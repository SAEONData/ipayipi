% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/z_h0_station_dt_process.R
\name{dt_process}
\alias{dt_process}
\title{Pipeline data: processing --- \emph{dt}}
\usage{
dt_process(
  station_file = NULL,
  pipe_house = NULL,
  pipe_seq = NULL,
  stages = NULL,
  output_dt_preffix = "dt_",
  output_dt_suffix = NULL,
  overwrite_pipe_memory = FALSE,
  verbose = FALSE,
  unwanted_tbls = "_tmp",
  xtra_v = FALSE,
  keep_open = TRUE,
  ...
)
}
\arguments{
\item{station_file}{Name of the station being processed.}

\item{pipe_house}{List of pipeline directories. \strong{See} \code{ipayipi::ipip_house()} \strong{for details}.}

\item{pipe_seq}{Generated processing pipeline structure performed by \code{ipayipi::pipe_seq()}. Defaults to \code{NULL}.
If this argument is not provided, then the \code{pipe_seq} stored in the station file will be used for data processing. When changes are made to a stations processing pipeline \code{overwrite_pipe_memory} should be set to \code{TRUE}, in which case a \code{pipe_seq} object must be provided here---this \code{pipe_seq} will be embedded in the station file.}

\item{output_dt_preffix}{The output-table preffix. Defaults is 'dt_'.}

\item{output_dt_suffix}{A custom suffix to be appended to the output tables name.}

\item{overwrite_pipe_memory}{Logical. If \code{TRUE} then extant pipeline steps, which are summarised in the 'pipe_process_summary' table (\emph{see details}), are modified by arguments in the pipe_process argument.}

\item{verbose}{Logical. Whether or not to report messages and progress.}

\item{unwanted_tbls}{Some tables generated by the processing pipeline don't need to be stored permanently in the station file object. By adding keywords to this argument (separated by the '|' character) these 'unwanted' (or temporary) tables will be removed from the station file. Defaults to \verb{_tmp} --- so any table with this search key in its name will be removed.}

\item{xtra_v}{Logical. Whether or not to report xtra messages, progess, plus print data tables.}

\item{keep_open}{Logical. If TRUE the station file connection is kept in the temporary directory after the function is finished processing.}

\item{cores}{Number of CPU's to use for processing in parallel. Only applies when working on Linux.}
}
\description{
Process data in sequential steps generated by \code{ipayipi::pipe_seq()}.
}
\details{
This function forms the basis of setting up a sequential, data-processing pipeline. In this process, raw or other data from an 'ipayipi' station file is harvested, and further processing of this data can be run.

The first part of the processing stage of the \code{ipayipi} data pipeline is to set up pipe stages, each with its own sequence of processing steps, using \code{ipayipi::pipe_seq()}. Once the sequence is set up it can be parsed to \code{ipayipi::dt_process()}. The four main functions that \code{ipayipi::dt_process()} uses to process data are:
\itemize{
\item \code{dt_harvest}: for harvesting station/other data.
\item \code{dt_calc_chain}: running \code{data.table} chained calculations on harvested data.
\item \code{dt_agg}: Aggregate phenomena/variables by custom or default functions.
Defaults are based on the phenomena descriptions in \code{phens} tables, i.e., their measure, variable type, and units. Default aggregation functions are housed in the \code{ipayipi::sts_agg_functions} table.
\item \code{dt_join}: Used to merge harvested data sets via simple or comlex 'fuzzy' type joins based on time intervals using \code{data.table}s join syntax, and ipayipi's own time-series sensitive data joining \code{ipayipi::append_phen_data()}, implemented through \code{ipayipi::mhlanga()}.
}

The above functions can be specified in the \code{pipe_seq} function. \code{pipe_seq} runs partial to fuller evaluation of pipeline structure to promote seemless processing. Fuller evaluation by \code{ipayipi::pipe_seq()} is performed within \code{ipayipi::dt_process()}; during this process, station data (both internally and externally harvested data) are read so new-phenomena metadata can be generated. All this to minimise potential error during data processing.
Processed data, function parameters, and new phenomena summaries are returned and appended to station files for future use.
}
\author{
Paul J. Gordijn
}
\keyword{data}
\keyword{pipeline;}
\keyword{processing}
\keyword{processing;}
\keyword{steps}
