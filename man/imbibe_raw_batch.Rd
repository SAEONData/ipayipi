% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/b_imbibe_raw_batch.R
\name{imbibe_raw_batch}
\alias{imbibe_raw_batch}
\title{Batch imbibe of 'flat' data files into the 'ipayipi' format}
\usage{
imbibe_raw_batch(
  pipe_house = NULL,
  wipe_source = FALSE,
  file_ext_in = NULL,
  file_ext_out = ".ipr",
  col_dlm = NULL,
  dt_format = c("Ymd HMOS", "Ymd HMS", "Ymd IMOSp", "Ymd IMSp", "ymd HMOS", "ymd HMS",
    "ymd IMOSp", "ymd IMSp", "mdY HMOS", "mdY HMS", "mdy HMOS", "mdy HMS", "mdy IMOSp",
    "mdy IMSp", "dmY HMOS", "dmY HMS", "dmy HMOS", "dmy HMS", "dmy IMOSp", "dmy IMSp"),
  dt_tz = "Africa/Johannesburg",
  record_interval_type = "continuous",
  remove_prompt = FALSE,
  max_rows = 1000,
  logg_interfere_type = "on_site",
  data_setup = NULL,
  prompt = FALSE,
  wanted = NULL,
  unwanted = NULL,
  recurr = FALSE,
  verbose = FALSE,
  xtra_v = FALSE,
  ...
)
}
\arguments{
\item{pipe_house}{List of pipeline directories. \emph{See} \code{\link[=ipip_house]{ipip_house()}} \emph{for details}.}

\item{wipe_source}{Logical. If \code{TRUE} then raw data files in the source location will be deleted. \emph{See details}}

\item{file_ext_in}{The file extension defaults of the raw logger data files. This can be left as \code{NULL} so 'all' files but those with extensions that cannot be imbibed (".ipr|.ipi|.iph|.xls|.rps|.rns|.ods|.doc").}

\item{file_ext_out}{The file extension used when raw logger data which has
been imbibed into the \code{ipayipi} data pipeline. Advisable to leave this as the default (".ipr") for the pipeline structure.}

\item{col_dlm}{The column delimter which is fed to \code{\link[data.table:fread]{data.table::fread()}}. Defaults to NULL. When \code{NULL} the function uses \code{data.table::fread} ability to 'guess' the delimeter.}

\item{dt_format}{The function attempts to work out the date-time format from a vector of format types supplied to this argument. The testing is done via \code{\link[lubridate:parse_date_time]{lubridate::parse_date_time()}}. \code{\link[lubridate:parse_date_time]{lubridate::parse_date_time()}} prioritizes the tesing of date-time formats in the order vector of formats supplied. The default vector of date-time formats supplied should work well for most logger outputs. \bold{NB!} seconds are required.}

\item{dt_tz}{Recognized time-zone (character string) of the data locale. The default for the package is South African, i.e., "Africa/Johannesburg" which is equivalent to "SAST".}

\item{record_interval_type}{If there are is no discrete record interval set in the logger program, i.e., the sampling is event-based, then this parameter must be set to "event_based". By default this function has this parameter set to "continuous", but the record interval is scrutinized by \code{\link[=record_interval_eval]{record_interval_eval()}} --- see the help files for this function for more information.
The parameter supplied here is only used if there is only one data record and the record interval cannot be evaluated by \code{\link[=record_interval_eval]{record_interval_eval()}}.}

\item{remove_prompt}{Logical; passed to \code{\link[=record_interval_eval]{record_interval_eval()}}. Activate a readline prompt to choose whether or not filter our records from \code{dta_in} with inconsistent record intervals.}

\item{max_rows}{The number of rows to use when evaluating the record interval. Argument is parsed to \code{\link[=record_interval_eval]{record_interval_eval()}}.}

\item{logg_interfere_type}{Two options here: "remote" or "on_site". Each time a logger is visited is counted as a logger interference event. Type \emph{'remote'} occurs when data is downloaded remotely. Type _'on_site'_is when data was downloaded on site. \emph{See} \code{\link[=imbibe_raw_logger_dt]{imbibe_raw_logger_dt()}}.}

\item{data_setup}{List of options used to extract data and metadata from instrument data outputs. These arguments are parsed to \code{\link[=imbibe_raw_flat]{imbibe_raw_flat()}}.}

\item{prompt}{Set to TRUE for interactive mode. Note this will not work if embedded in a parallel processing instance.}

\item{wanted}{Regex string of files to select for listing. Seperate search tags by using the bar character '|'.}

\item{unwanted}{Regex string of files to filter out the listing. Seperate search tags by using the bar character '|'.
in the import.}

\item{recurr}{Should the function search recursively i.e., thorugh sub-folders as well - \code{TRUE}/\code{FALSE.}}

\item{verbose}{Logical. Print some details and progress of function progress?}

\item{xtra_v}{Logical. Should some 'x'tra messaging be done? Use to help diagnose problems, and for guidance.}
}
\description{
Reads and transfers data files to the begining stages of the \code{ipayipi} data pipeline. Option to archive all 'raw' data files in the pipeline structure, \emph{see details}.
}
\details{
\subsection{Custom data setup options:}{

Custom data setups include those for hobo rainfall flat files \link{hobo_rain}, generic flat data, solonist xml files \link{solonist}, Cambell Scientific TOA5 data 'dat' files \link{cs_toa5}, and data from the SAEON Terrestrial Observations Monitor \link{cs_stom}. Take a look at these as examples when setting up your own data setup options following the guidelines outlined in \code{\link[=imbibe_raw_flat]{imbibe_raw_flat()}}.
}

\subsection{'Archiving' raw data}{

Files brought into the 'd2_wait_room' are only housed there temporarily. In order to archive these raw data files a 'd0_raw_room' directory must be provided in the 'pipe_house' object (\emph{see} \code{\link[=ipip_house]{ipip_house()}}). Files will be archived in structured directories in the 'd0_raw_room' named by the last year and month in their respective date time records. Original  file names are maintained, and have a suffix with a unique integer plus the date and time of which they were archived. N.B. Files in the source directory (\code{source_dir}) are only deleted when \code{wipe_source} is set to \code{TRUE}.
}

\subsection{Reading flat files}{

This function uses \link[data.table:fread]{data.table::fread} for flat text files, which is optimized for processing 'big data'. Apart from usual the usual options which can be parsed to \code{data.table::fread} this function generates some standardised metadata to complement the read from a logger data table (if \code{data.table::fread()} is unsuccessful \code{base::read.csv()} is used). This metadata may vary from one logger output to another. To cater for this variation this function requires a \code{data_setup} to be completed. Once setup this can be used as a standard for further imports.
There is support for 'xml' formatted Solonist 'xle' format files with a default \code{data_setup} that is parsed automatically for files with the '.xle' extension.
}

\subsection{Record interval}{

This function attempts to check whether the recording interval in the data date-time stamp has been consistent. A prompt is called if there are inconsistent time intervals between record events, and data rows with inconsistent time intervals will be removed if approved.
A basic check is performed to check the success of converting date-time values to a recognised format in R (i.e., POSIXct).
}

\subsection{Logger interference events:}{

Regarding the \code{logg_interfere_type} parameter. Owing to potential interference of sensors etc when downloading data 'on site' or logger related issues when data is sent/obtained remotely, the date-time stamps of these events must be preserved. A \code{logg_interfere} data table is generated for this purpose and stored with the data. This data cannot necessarily be extracted from the 'data_summary' once data has been appended as some of this data will be overwritten during the appending process. The purpose of the 'logg_interfere' table is to retain this information, which is used by \code{ipayipi} for further processing.
}

\subsection{Parallel processing}{

This function can run considerably faster in parallel. ipayipi uses the \link{future} and \link{future.apply} libraries for parallel processing. \strong{See} \code{\link[future:plan]{future::plan()}} for setting up your parallel processing options that can be implemented in ipayipi.
}
}
\author{
Paul J. Gordijn
}
\keyword{automatic}
\keyword{batch}
\keyword{data}
\keyword{data;}
\keyword{meteorological}
\keyword{pipeline}
\keyword{process;}
\keyword{raw}
\keyword{standardisation;}
\keyword{station;}
\keyword{weather}
