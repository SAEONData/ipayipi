#' @title Pipeline data processing: _dt_
#' @description Process data in sequential steps generated by
#'  `ipayipi::pipe_seq()`.
#' @param station_file Name of the station being processed.
#' @param pipe_house List of pipeline directories. __See__ `ipayipi::ipip_init()` __for details__.
#' @param pipe_seq Generated processing pipeline structure performed by
#'  `ipayipi::pipe_seq()`.
#' @param output_dt_preffix The output table preffix which defaults to
#'  "dt_".
#' @param output_dt_suffix A custom suffix to be appended to the output
#'  tables name.
#' @param overwrite_pipe_memory Logical. If TRUE then extant pipeline steps,
#'  which are summarised in the 'pipe_process_summary' data table (*see
#'  details*), are modified by arguments in the pipe_process argument.
#' @author Paul J. Gordijn
#' @keywords data pipeline; data processing; processing steps
#' @details This function forms the basis of setting up a sequential data
#'  processing pipeline. This allows the extraction and preparation of
#'  raw, or other data from a data table in an 'ipayipi' station file,
#'  and further processing of this data.
#'
#'  The first part of the processing stage of the `ipayipi` data pipeline
#'  is to set up a pipe stage and step sequence using `ipayipi::pipe_seq()`.
#'  Once the sequence is set up it can be parsed to `ipayipi::dt_process()`.
#'  The four main functions that `ipayipi::dt_process()` uses to process data
#'  are:
#'  - `dt_harvest`: for harvesting station/other data.
#'  - `dt_calc_chain`: running `data.table` chained calculations on data.
#'  - `dt_agg`: Aggregate phenomena/variables by custom or default functions.
#'    Defaults are based on the phenomena descriptions in `phens` tables, i.e.,
#'    their measure, variable type, and units.
#'  - `dt_join`: Used to merge harvested data sets together via simple and
#'    more comlex fuzzy type joins using `data.table`. _Not yet implemented._
#'  These functions can be specified in the `pipe_seq` function.
#'  `pipe_seq` will itself run some basic checks on the pipeline structure and
#'  to help ensure smooth running of the processing. More complex checks on the
#'  structure of the pipeline are done in the `dt_process` function using
#'  full evaluation of the functions mentioned above. During this process
#'  station data (both external for external harvesting) and the station wherein
#'  data are being processed are opened and new phenomena descriptions are
#'  generated. Function parameters are also generated for each of the functions
#'  above. All this to minimise potential error during the actual data
#'  processing performed by the functions above.
#'  Processed data, function parameters, and new phenomena summariies are
#'  returned and appended to station files for future use.
#'
#' @export
dt_process <- function(
  station_file = NULL,
  pipe_house = NULL,
  pipe_seq = NULL,
  output_dt_preffix = "dt_",
  output_dt_suffix = NULL,
  overwrite_pipe_memory = TRUE,
  ...
) {
  "%ilike%" <- "dt_n" <- "dtp_n" <- "ppsid" <- "phen_name" <- "ii" <-
    "jj" <- ".N" <- NULL

  # open station file connection
  sfc <- ipayipi::open_sf_con(pipe_house = pipe_house, station_file =
    station_file)

  # read function summary tables
  # open output_dt and associate table summary
  sf_names <- names(sfc)
  f_summary <- ipayipi::sf_read(pipe_house = pipe_house, sfc = sfc, tmp = TRUE,
    tv = sf_names[sf_names %ilike% "summary|phens|pipe_seq"],
    station_file = station_file)
  f_summary$sf_names <- sf_names
  dt_names <- sf_names[sf_names %ilike% output_dt_preffix]

  # get dttm max min dates
  sf_slice <- lapply(seq_along(dt_names), function(i) {
    return(list(
      sf_min = min(sf[[dt_names[i]]]$date_time),
      sf_max = max(sf[[dt_names[i]]]$date_time))
    )
  })
  names(sf_slice) <- dt_names

  ## standardise the overall pipe process summary
  pps <- f_summary$pipe_seq
  pp <- pipe_process(pipe_seq = pipe_seq, pipe_memory = pps)

  if (pp$update_pipe_data) {
    # prep pps for partial evaluation
    pps <- pp$pipe_seq
    pps <- split(pps, f = factor(pps$dt_n))
  } else {# no eval if NULL --- sf already has pps
    pps <- NULL
  }

  # save station file in a temporary location
  #sf_tmp_fn <- tempfile(pattern = "ipip_", tmpdir = tempdir())
  #saveRDS(sf, file = sf_tmp_fn)

  # pps full evaluation ----
  # set up paired functions
  ff <- list(
    list("dt_harvest", "dt_calc", "dt_agg", "dt_join"),
    list("hsf_param_eval", "calc_param_eval", "agg_param_eval",
      "join_param_eval"),
    list("hsf_params", "calc_params", "agg_params", "join_params")
  )
  cr_msg <- padr(core_message =
      paste0(" Processing data ", collapes = ""),
    wdth = 80, pad_char = "=", pad_extras = c("|", "", "", "|"),
    force_extras = FALSE, justf = c(0, 0))
  message(cr_msg)

  # pipeline evaluation -------------------------------------------------------
  # full evaluation of the pipeline then save evaluated f_params to function
  #  tables

  # temp dir name for working files
  #o_temp <- tempfile(pattern = "oipip_", tmpdir = tempdir())
  #saveRDS(list(dummy = NULL), o_temp)
  # work through stages of pipe evaluation
  ii <- lapply(seq_along(pps), function(i) {
    ppsi <- pps[[i]]
    #sf <- readRDS(sf_tmp_fn)
    f_summary <- f_summary[sf_names %ilike% "summary|phens"]
    f_summary$sf_names <- sf_names
    jj <- lapply(seq_along(unique(ppsi$dtp_n)), function(j) {
      # read in previous temp data
      #otmp <- readRDS(o_temp)
      # get function and prepare arguments
      ppsij <- ppsi[dt_n == i & dtp_n == j, ]
      f <- ppsij$f[1]
      f <- ff[[2]][ff[[1]] %in% f][[1]]
      if (!f %in% c("calc_param_eval", "agg_param_eval")) {
        f_params <- eval(parse(text = ppsij$f_params[1]))
        class(f_params) <- c("f_params", "list")
      } else {
        f_params <- NULL
      }
      args <- list(
        station_file = station_file,
        f_params = f_params,
        ppsij = ppsij,
        full_eval = TRUE,
        sfc = sfc
        #f_summary = f_summary,
        #sf_tmp_fn = sf_tmp_fn,
        #eval_seq = otmp,
        #sf = sf
      )
      o <- do.call(what = f, args = args)
      o <- o[!sapply(o, is.null)]
      # read in and append 'o' data where possible
      fo <- o$f_params
      sfc_f_params <- ipayipi::sf_read(sfc = sfc, tv = "f_params",
        pipe_house = pipe_house, station_file = station_file, tmp = TRUE)
      if (length(sfc_f_params) == 0) {
        sfc_f_params <- NULL
      }
      o <- o[!names(o) %in% "f_params"]
      # fotmp <- otmp$f_params
      otmp <- otmp[!names(otmp) %in% "f_params"]
      o <- ipayipi::append_tables(original_tbl = otmp, new_tbl = o)
      fo <- ipayipi::append_tables(original_tbl = sfc_f_params, new_tbl = fo)
      # o <- o[!names(o) %in% "dummy"]
      o$phens_dt <- o$phens_dt[order(ppsid, phen_name)]
      o <- list(f_params = fo, phens_dt = o$phens_dt)
      lapply(names(o), function(x) {
        saveRDS(o[[x]], file.path(dirname(sfc[1]), x))
      })
      return(o)
    })
    rm(jj)
  })
  if (!is.null(pps)) {
    # add pipe eval xtras onto the temp station file
    otmp <- readRDS(o_temp)
    sf <- sf[!names(sf) %in% names(otmp)]
    sf <- append(sf, otmp)
    saveRDS(sf, sf_tmp_fn)
    sf <- readRDS(sf_tmp_fn)
  }
  pps <- sf$pipe_seq
  pps <- split(pps, f = factor(pps$dt_n))

  # process data --------------------------------------------------------------
  dttmp <- file.path(paste0(tempdir(), "dt_working_",
    basename(station_file)), collapse = "")
  dir.create(dttmp, showWarnings = FALSE)
  dt_working_n <- list.files(path = dttmp, full.names = TRUE)
  lapply(dt_working_n, unlink)
  # run functions using pps and other info collected in the function eval
  ii <- lapply(seq_along(pps), function(i) {
    dt_working_n <- list.files(path = dttmp, full.names = TRUE)
    lapply(dt_working_n, unlink)
    sf <- readRDS(sf_tmp_fn)
    ppsi <- pps[[i]]
    jj <- lapply(seq_along(1:3), function(j) {
      # get function and prepare arguments
      ppsij <- ppsi[dt_n == i & dtp_n == j, ]
      f <- ppsij$f[1]
      if (!f %in% c("dt_calc", "dt_join")) {
        f_params <- sf[["f_params"]][[ff[[3]][ff[[1]] %in% f][[1]]]][
          ppsid == paste0(i, "_", j)]
      } else {
        f_params <- as.list(ppsij$f_params)
      }
      # read into memory any files in the temp dir
      dt_working_n <- list.files(path = dttmp, full.names = TRUE)
      dt_working <- lapply(dt_working_n, function(x) {
        d <- readRDS(x)
        return(d)
      })
      dt_working <- unlist(dt_working, recursive = FALSE)
      cr_msg <- padr(core_message =
          paste0(f, collapes = ""),
        wdth = 80, pad_char = " ", pad_extras = c("|", "", "", "|"),
        force_extras = FALSE, justf = c(1, 3))
      message(cr_msg)
      # get arguments and process function
      args <- list(
        station_file = station_file,
        f_params = f_params,
        f_summary = f_summary,
        phens_dt = sf$phens_dt,
        sf_tmp_fn = sf_tmp_fn,
        ppsij = ppsij,
        sf = sf,
        dt_working = dt_working
      )
      dti <- do.call(what = f, args = args)
      saveRDS(dti, file = file.path(dttmp, names(dti)[1]))
      # remove harvested data if funtion not "dt_harvest"
      if (!f %in% "dt_harvest") {
        sapply(dt_working_n[dt_working_n %ilike% "hsf_dts"], unlink)
      }
    })
    rm(jj)

    # append files to the temporary station file and save
    dt_working_n <- list.files(path = dttmp, full.names = TRUE)
    dt_working <- lapply(dt_working_n, function(x) {
      d <- readRDS(x)
      return(d)
    })
    dt_working <- unlist(dt_working, recursive = FALSE)
    dt <- dt_working[names(dt_working) %ilike% "dt"][1]
    names(dt) <- ppsi[.N]$output_dt
    sf <- append(sf, dt)
    saveRDS(sf, file = sf_tmp_fn)
  })
  rm(ii)
  sf <- readRDS(sf_tmp_fn)
  saveRDS(sf, station_file)
  cr_msg <- padr(core_message =
      paste0("", collapes = ""),
    wdth = 80, pad_char = "=", pad_extras = c("|", "", "", "|"),
    force_extras = FALSE, justf = c(-1, 0))
  message(cr_msg)
  return(sf)
}