#' @title Pipeline data processing: _dt_
#' @description Process data in sequential steps generated by
#'  `ipayipi::pipe_seq()`.
#' @param station_file Name of the station being processed.
#' @param pipe_house List of pipeline directories. __See__ `ipayipi::ipip_init()` __for details__.
#' @param pipe_seq Generated processing pipeline structure performed by
#'  `ipayipi::pipe_seq()`.
#' @param output_dt_preffix The output table preffix which defaults to
#'  "dt_".
#' @param output_dt_suffix A custom suffix to be appended to the output
#'  tables name.
#' @param overwrite_pipe_memory Logical. If TRUE then extant pipeline steps,
#'  which are summarised in the 'pipe_process_summary' data table (*see
#'  details*), are modified by arguments in the pipe_process argument.
#' @author Paul J. Gordijn
#' @keywords data pipeline; data processing; processing steps
#' @details This function forms the basis of setting up a sequential data
#'  processing pipeline. This allows the extraction and preparation of
#'  raw, or other data from a data table in an 'ipayipi' station file,
#'  and further processing of this data.
#'
#'  The first part of the processing stage of the `ipayipi` data pipeline
#'  is to set up a pipe stage and step sequence using `ipayipi::pipe_seq()`.
#'  Once the sequence is set up it can be parsed to `ipayipi::dt_process()`.
#'  The four main functions that `ipayipi::dt_process()` uses to process data
#'  are:
#'  - `dt_harvest`: for harvesting station/other data.
#'  - `dt_calc_chain`: running `data.table` chained calculations on data.
#'  - `dt_agg`: Aggregate phenomena/variables by custom or default functions.
#'    Defaults are based on the phenomena descriptions in `phens` tables, i.e.,
#'    their measure, variable type, and units.
#'  - `dt_join`: Used to merge harvested data sets together via simple and
#'    more comlex fuzzy type joins using `data.table`. _Not yet implemented._
#'  These functions can be specified in the `pipe_seq` function.
#'  `pipe_seq` will itself run some basic checks on the pipeline structure and
#'  to help ensure smooth running of the processing. More complex checks on the
#'  structure of the pipeline are done in the `dt_process` function using
#'  full evaluation of the functions mentioned above. During this process
#'  station data (both external for external harvesting) and the station wherein
#'  data are being processed are opened and new phenomena descriptions are
#'  generated. Function parameters are also generated for each of the functions
#'  above. All this to minimise potential error during the actual data
#'  processing performed by the functions above.
#'  Processed data, function parameters, and new phenomena summariies are
#'  returned and appended to station files for future use.
#'
#' @export
dt_process <- function(
  station_file = NULL,
  pipe_house = NULL,
  pipe_seq = NULL,
  output_dt_preffix = "dt_",
  output_dt_suffix = NULL,
  overwrite_pipe_memory = TRUE,
  ...
) {
  "%ilike%" <- "dt_n" <- "dtp_n" <- "ppsid" <- "phen_name" <- "ii" <-
    "jj" <- ".N" <- NULL

  # open station file connection
  sfc <- ipayipi::open_sf_con(pipe_house = pipe_house, station_file =
    station_file)

  # read function summary tables
  # open output_dt and associate table summary
  sf_names <- names(sfc)
  f_summary <- ipayipi::sf_read(pipe_house = pipe_house, sfc = sfc, tmp = TRUE,
    tv = sf_names[sf_names %ilike% "summary|phens|pipe_seq"],
    station_file = station_file)
  f_summary$sf_names <- sf_names
  dt_names <- sf_names[sf_names %ilike% output_dt_preffix]

  # get dttm max min dates
  sf_slice <- lapply(seq_along(dt_names), function(i) {
    dt_working <- ipayipi::sf_read(sfc = sfc, tmp = TRUE, tv = dt_names[i])[[
      dt_names[i]
    ]]
    return(list(
      sf_min = min(dt_working$date_time),
      sf_max = max(dt_working$date_time))
    )
  })
  names(sf_slice) <- dt_names

  ## standardise the overall pipe process summary
  pps <- f_summary$pipe_seq
  pp <- pipe_process(pipe_seq = pipe_seq, pipe_memory = pps)

  if (pp$update_pipe_data) {
    # prep pps for partial evaluation
    pps <- pp$pipe_seq
    pps <- split(pps, f = factor(pps$dt_n))
  } else {# no eval if NULL --- sf already has pps
    pps <- NULL
  }


  # pps full evaluation ----
  # set up paired functions
  ff <- list(
    list("dt_harvest", "dt_calc", "dt_agg", "dt_join"),
    list("hsf_param_eval", "calc_param_eval", "agg_param_eval",
      "join_param_eval"),
    list("hsf_params", "calc_params", "agg_params", "join_params")
  )
  cr_msg <- padr(core_message =
      paste0(" Processing data ", collapes = ""),
    wdth = 80, pad_char = "=", pad_extras = c("|", "", "", "|"),
    force_extras = FALSE, justf = c(0, 0))
  message(cr_msg)

  # pipeline evaluation -------------------------------------------------------
  # full evaluation of the pipeline then save evaluated f_params to function
  #  tables

  ii <- lapply(seq_along(pps), function(i) {
    print(i)
    ppsi <- pps[[i]]
    jj <- lapply(seq_along(unique(ppsi$dtp_n)), function(j) {
      sfc <- open_sf_con(pipe_house = pipe_house, station_file = station_file)
      print(j)
      # get function and prepare arguments
      ppsij <- ppsi[dt_n == i & dtp_n == j, ]
      f <- ppsij$f[1]
      f <- ff[[2]][ff[[1]] %in% f][[1]]
      if (!f %in% c("calc_param_eval", "agg_param_eval")) {
        f_params <- eval(parse(text = ppsij$f_params[1]))
        class(f_params) <- c("f_params", "list")
      } else {
        f_params <- NULL
      }
      args <- list(
        station_file = station_file,
        f_params = f_params,
        ppsij = ppsij,
        full_eval = TRUE,
        sfc = sfc
      )
      o <- do.call(what = f, args = args)

      fo <- o$f_params
      sfc_f_params <- ipayipi::sf_read(sfc = sfc, tv = "f_params",
        pipe_house = pipe_house, station_file = station_file, tmp = TRUE,
        verbose = FALSE)
      sfc_f_params_n <- names(sfc_f_params$f_params)
      if (length(sfc_f_params) == 0) sfc_f_params <- NULL
      sfc_f_params <- unlist(sfc_f_params, recursive = FALSE)
      names(sfc_f_params) <- sfc_f_params_n
      o <- o[!names(o) %in% "f_params"]
      oo <- ipayipi::sf_read(sfc = sfc, tv = names(o),
        pipe_house = pipe_house, station_file = station_file, tmp = TRUE)
      if (length(oo) == 0) oo <- NULL
      o <- ipayipi::append_tables(original_tbl = oo, new_tbl = o)
      fo <- ipayipi::append_tables(original_tbl = sfc_f_params, new_tbl = fo)
      if (!is.null(o$phens_dt)) {
        o$phens_dt <- o$phens_dt[order(ppsid, phen_name)]
      }
      fo <- fo[!sapply(fo, is.null)]
      o <- list(f_params = fo, phens_dt = o$phens_dt)
      lapply(names(o), function(x) {
        saveRDS(o[[x]], file.path(dirname(sfc[1]), x))
      })
      return(o)
    })
    rm(jj)
  })
  sfc <- open_sf_con(pipe_house = pipe_house, station_file = station_file)

  # process data --------------------------------------------------------------
  ii <- lapply(seq_along(pps), function(i) {
    ppsi <- pps[[i]]
    sfc <- open_sf_con(pipe_house = pipe_house, station_file = station_file)
    if ("dt_working" %in% names(sfc)) {
      unlink(sfc["dt_working"], recursive = TRUE)
    }
    #print(i)seq_along(unique(ppsi$dtp_n))
    jj <- lapply(seq_along(unique(ppsi$dtp_n)), function(j) {
      #print(j)
      sfc <- open_sf_con(pipe_house = pipe_house, station_file = station_file)
      # get function and prepare arguments
      ppsij <- ppsi[dt_n == i & dtp_n == j, ]
      f <- ppsij$f[1]
      fpm <- ipayipi::sf_read(sfc = sfc, tv = "f_params", tmp = TRUE,
        station_file = station_file)
      if (!f %in% c("dt_calc", "dt_join")) {
        f_params <- fpm[["f_params"]][[ff[[3]][ff[[1]] %in% f][[1]]]][
          ppsid == paste0(i, "_", j)]
      } else {
        f_params <- as.list(ppsij$f_params)
      }
      cr_msg <- padr(core_message =
          paste0(f, collapes = ""),
        wdth = 80, pad_char = " ", pad_extras = c("|", "", "", "|"),
        force_extras = FALSE, justf = c(1, 3))
      message(cr_msg)
      # get arguments and process function
      args <- list(
        station_file = station_file,
        f_params = f_params,
        ppsij = ppsij,
        sfc = sfc
      )
      dti <- do.call(what = f, args = args)
      # if there is hsf data and no dt working create dt working table
      if (any(names(dti) %in% "hsf_dts") &&
        !any(names(sfc) %in% "dt_working")) {
          dl <- length(dti$hsf_dts)
          sapply(seq_len(dl - 1), function(xi) {
            warning("More than one harvested dataset. Using latest harvest.")
          })
          dti$dt_working <- dti$hsf_dts[[length(dti$hsf_dts)]]
      }
      lapply(names(dti), function(x) {
        saveRDS(dti[[x]], file.path(dirname(sfc[1]), x))
      })

    })
    sfc <- open_sf_con(pipe_house = pipe_house, station_file = station_file)
    # clean up the harvest files
    sapply(names(sfc)[names(sfc) %in% "hsf_dts"], function(x) {
      unlink(sfc[x], recursive = TRUE)
    })
    # convert dt_working to output_dt
    odtn <- unique(ppsi$output_dt)
    file.rename(sfc["dt_working"], file.path(dirname(sfc["dt_working"]), odtn))
  })
  rm(ii)

  cr_msg <- padr(core_message =
      paste0("", collapes = ""),
    wdth = 80, pad_char = "=", pad_extras = c("|", "", "", "|"),
    force_extras = FALSE, justf = c(-1, 0))
  message(cr_msg)
  return(sf)
}