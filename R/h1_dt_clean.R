#' @title Dev function: Apply filter windows to time series data
#' @description Function under development. Uses the non-parametric hampel filter to detect anomalies and impute values in a univariate series of numeric data.
#' @param phens Vector of the phenomena names that will be evaluated by the hampel filter. If NULL the function will not run.
#' @param w Window size for the hempel filter. Defaults to ten.
#' @param madf Scalar factor of MAD (median absolute deviation). Higher values relax oulier detection. Defaults to the standard of three.
#' @param align Corresponds to data.table's frollapply function's 'align' argument. Options: 'right' -- value being evaluated is on the right with respect to preceeding values; 'left' -- opposite of 'right'; 'centre' -- value under evaluation mid window (if even, biased to the left).
#' @param seg Name of column to segment filtering on. Cleaning within each data segment or slice will be run independently.
#' @param cush Logical indicating whether to prevent NAs at data start and end dates owing to window/filter alignment. `TRUE` will \bold{cush}ion data start and ends where necessary by adjust window alignment properties (within segments). `FALSE` does not prevent NA values.
#' @param clean_f Algorithm name. Only "hampel" supported.
#' @param owrite Logical. Should outliers be overwritten in the data? In any case a table of original and replacement values is generated by this function.
#' @param na_t Threshold (as a proportion of window length) of NA values tolerated. If the number of NAs exceeds the threshold, filtering will not be performed. Must be supplied as a vector or list with subsequent values corresponding to each filter 'run'.
#' @param tighten Not yet implemented. Scaling fraction between zero and one that sensitizes the detection of outliers near the head and tail ends of segments. The fraction is multiplied by the mad factor.
#' @param station_file Name of the station being processed.
#' @param pipe_house If the `pipe_house` argument is provided the `pipe_house$ipip_room` will be used instead of the `input_dir`.
#' @param sfc  List of file paths to the temporary station file directory. Generated using `ipayipi::sf_open_con()`.
#' @param ppsij Data processing `pipe_seq` table from which function parameters and data are extracted/evaluated. This is parsed to this function automatically by `ipayipi::dt_process()`.
#' @param f_params Function parameters evaluated by `ipayipi::calc_param_eval()`. These are internally parsed to `dt_calc()` from `dt_process()`.
#' @param verbose Logical. Whether or not to report messages and progress.
#' @param xtra_v Logical. Whether or not to report xtra messages, progess, plus print data tables.
#' @param chunk_v Logical. Print data chunking messages. Useful for debugging/digging into chunking methods.
#' @param seg_fuzz Not yet implemented. String representing the threshold time interval between the list of segment date-time 
#' @keywords outlier detection, value imputation, univariate data,
#' @details This function employs a user-customised hampel filter to check data for outliers and impute missing values with the median of the filter window. Function under development.
#' - Need to add generated table details to the phen_ds summary table!
#' @export
#' @author Paul J. Gordijn
dt_clean <- function(
  phens = NULL,
  w = 21,
  madf = 3,
  align = "left",
  seg = NULL,
  cush = TRUE,
  clean_f = "hampel",
  owrite = TRUE,
  na_t = 0.25,
  tighten = 0.65,
  station_file = NULL,
  pipe_house = NULL,
  sfc = NULL,
  ppsij = NULL,
  f_params = NULL,
  verbose = FALSE,
  xtra_v = FALSE,
  chunk_v = FALSE,
  ...
) {

  "%ilike%" <- "%chin%" <- ":=" <- "." <- ".N" <- NULL
  "table_name" <- "flag" <- "seg_dflt" <- "date_time" <- "n" <- "original_v" <-
    "replace_v" <- "phen" <- "s" <- "stage" <- NULL
  # set default args (f_params) from ppsij
  #  - generate a table for this purpose
  # determine how far back to reach when opening data
  # open data
  #  - filter bt start and end dttm
  #  - check for phens
  #  - subset data by phens
  # loop through runs with froll
  #  - generate outlier dt tbl for each sequential loop
  #    this needs to be sequential where the next run depends on
  #    a cleaned version of the last [do while]
  # perform the tighten arg
  # save data and outlier dt tbl

  # prep data read ----
  sfcn <- names(sfc)
  dta_in <- NULL
  hsf_dta <- NULL
  if ("dt_working" %in% sfcn) {
    hsf_dta <- "dt_working"
    dta_in <- sf_dta_read(sfc = sfc, tv = "dt_working")
    ng <- sf_dta_read(sfc = sfc, tv = "gaps")[["gaps"]][
      table_name %in% ppsij$output_dt[1]
    ]
  }

  if (is.null(dta_in) && any(sfcn %ilike% "_hsf_table_")) {
    pstep <- paste0("^", ppsij$dt_n[1], "_.+_hsf_table_")
    hsf_dta <- sfcn[sfcn %ilike% pstep]
    hsf_dta <- hsf_dta[order(as.integer(
      sapply(strsplit(hsf_dta, "_"), function(x) x[2])
    ))]
    hsf_dta <- hsf_dta[length(hsf_dta)]
    dta_in <- sf_dta_read(sfc = sfc, tv = hsf_dta)
    print(class(dta_in[[1]]))
    ng <- dta_in[[1]]$gaps
    ng$table_name <- ppsij$output_dt[1]
  }

  # eindx filter
  # *need to build in an extra filter for moving the window back
  dta_in_o <- dt_dta_filter(dta_link = dta_in, ppsij = ppsij)
  dta_in <- dta_in_o

  # prep clean args ----
  z_default <- data.table::data.table(table_name = NA_character_,
    phens = NA_character_, seg = NULL, madf = madf,
    w = NA_integer_, clean_f = NA_character_
  )[0]
  z <- lapply(f_params, function(x) {
    x <- eval(parse(text = sub("^~", "data.table::data.table", x)))
    x
  })
  z <- rbind(z_default, data.table::rbindlist(z, fill = TRUE), fill = TRUE)[
    order(phens)
  ]
  z$table_name <- ppsij$output_dt
  z$madf <- data.table::fifelse(is.na(z$madf), madf, z$madf)
  z$w <- data.table::fifelse(is.na(z$w), w, z$w)
  z$clean_f <- data.table::fifelse(is.na(z$clean_f), clean_f, z$clean_f)
  p <- c("date_time", z$phens)
  if (is.null(z$seg)) z$seg <- NA_character_
  segs <- z[!is.na(seg)]$seg
  # prep centre spelling for data.table
  z[align %ilike% "centre|center", "align"] <- "center"
  # set overwrite capability
  if (is.null(z$owrite)) {
    z$owrite <- owrite
  } else {
    owrite <- any(z$owrite)
  }
  # split by phen so each phen is evaluated seperately
  z <- split.data.frame(z, f = factor(z$phens))

  # open data ----
  dt <- dt_dta_open(dta_link = dta_in[[1]])
  dt <- subset(dt, select = c(unique(p), unique(segs)))
  if (chunk_v || xtra_v) {
    cli::cli_inform(c("i" = "Pre-clean data -- head"))
    print(head(dt))
  }

  # set default segment of whole data set
  dt[c(1, .N), seg_dflt := 1]

  tmpr <- file.path(tempfile(pattern = "cleanr"), station_file)
  dir.create(tmpr, recursive = TRUE)

  # run filters ----
  zouts <- lapply(z, function(zx) {
    phenx <- zx$phens[1]
    # set to default seg is none is defined
    zx[is.na(seg), ]$seg <- "seg_dflt"
    # save relevant data
    t <- dt[, names(dt)[names(dt) %chin% c("date_time", phenx, unique(zx$seg))],
      with = FALSE
    ]
    t[, p := phenx, env = list(phenx = phenx)]
    saveRDS(t, file.path(tmpr, phenx))
    series_mad <- mad_f(t[["p"]])
    lapply(seq_along(nrow(zx)), function(zi) {
      # read data
      t <- readRDS(file.path(tmpr, phenx))
      t <- t[, names(t)[!names(t) %chin% c("seg", "segn")], with = FALSE]
      t[c(1, .N), seg := 0]
      t[, seg := data.table::fifelse(!is.na(s), 0, seg),
        env = list(s = zx[zi]$seg)
      ]
      t[!is.na(seg), seg := seq_len(.N)]
      # divide into segments before checking outliers
      seq_l <- max(t$seg, na.rm = TRUE) - 1
      dtj <- lapply(seq_len(seq_l), function(j) {
        r1 <- which(t$seg %in% j)
        r2 <- which(t$seg %in% (j + 1)) - 1
        if (j == seq_l) r2 <- r2 + 1
        dtj <- t[r1:r2]
        dtj <- dtj[, n := j]
        return(dtj)
      })
      ### work segments with hampel filter ----
      dtj <- lapply(dtj, function(xj) {
        xj[, flag := data.table::frollapply(
          p, FUN = function(x) {
            hampel(x, w = zx$w[zi], d = zx$madf[zi], align = zx[zi]$align,
              series_mad = series_mad
            )
          }, n = zx$w[zi], align = zx[zi]$align
        )]
        #### cushion edges ----
        # if left cushion tail with right
        # if right cushion head with left
        # if centre cushion head and tail

        # setup window size for center cushions
        if (zx[zi]$align %in% "center") {
          ws <- zx$w[zi] / 2
          if (round(ws) != ws) ws <- (zx$w[zi] + 1) / 2
        } else {
          ws <- zx$w[zi]
        }
        # only cushion if enough rows in segment
        # set up cushion rows for evaluation
        if (nrow(xj) > (2 * ws) && cush == TRUE) {
          if (zx[zi]$align %in% "right") {
            r2 <- 0
          } else {
            r2 <- c((nrow(xj) - (ws * 2)):nrow(xj))
          }
          if (zx[zi]$align %in% "left") {
            r1 <- 0
          } else {
            r1 <- seq_len(ws * 2)
          }
        } else {
          r1 <- 0
          r2 <- 0
        }
        xj <- xj[r1, cush := data.table::frollapply(
          p, FUN = function(x) {
            hampel(x, w = zx$w[zi], d = zx$madf[zi], align = "left",
              series_mad = series_mad
            )
          }, n = zx$w[zi], align = "left"
        )]
        xj <- xj[r2, cush := data.table::frollapply(
          p, FUN = function(x) {
            hampel(x, w = zx$w[zi], d = zx$madf[zi], align = "right",
              series_mad = series_mad
            )
          }, n = zx$w[zi], align = "right"
        )]
        # generate summary table
        # replace values if option is specified
        xj <- xj[,
          p := data.table::fifelse(!is.na(flag), flag, p)
        ][,
          p := data.table::fifelse(!is.na(cush), cush, p)
        ][, -c("cush", "flag"), with = FALSE]
      })
      dtj <- data.table::rbindlist(dtj)
      dtj <- dtj[, c("date_time", phenx, unique(zx[zi]$seg), "p"), with = FALSE]
      saveRDS(dtj, file.path(tmpr, phenx))
    })
    return(phenx)
  })
  # organise outlier/replacement value summary tables
  zouts <- lapply(seq_along(zouts), function(i) {
    phenx <- names(zouts)[i]
    t <- readRDS(file.path(tmpr, phenx))
    ti <- t[, c("p")]
    t <- t[phenx != p, env = list(phenx = phenx)
    ][, ":="(stage = ppsij$dt_n[1], step = ppsij$dtp_n[1],
        original_v = phenx, replace_v = p,
        phen = names(zouts)[i]
      ), env = list(phenx = phenx)
    ][, .(date_time, step, stage, phen, original_v, replace_v)]
    data.table::setnames(ti, "p", phenx)
    return(list(zouts = t, owrite = ti))
  })
  names(zouts) <- names(z)
  # unlink tmp files
  unlink(tmpr, recursive = TRUE)

  # replace original values if requested (owrite = T)
  tn <- ppsij$output_dt[1]
  # overwrite data
  # get old data
  dt <- dt_dta_open(dta_link = dta_in[[1]])
  if (owrite) {
    # get new data
    dt_new <- do.call(cbind, lapply(zouts, function(x) x$owrite))
    names(dt_new) <- names(z)
    corder <- names(dt)
    dt <- dt[, names(dt)[!names(dt) %chin% names(z)], with = FALSE]
    dt <- cbind(dt, dt_new)
    data.table::setcolorder(dt, neworder = corder)
    # write data
  }
  if (chunk_v) cli::cli_inform(c(" " = "Chunking  working data"))
  sf_dta_rm(sfc = sfc, rm = "dt_working")
  sl <- sf_dta_wr(dta_room = file.path(dirname((sfc[1])), "dt_working"),
    dta = dt, overwrite = TRUE, tn = "dt_working",
    ri = ppsij[1]$time_interval, chunk_v = chunk_v
  )
  sf_log_update(sfc = sfc, log_update = sl)

  # write outlier table

  # save outlier/filter table to station ----
  # read in old table if it exists
  tn <- paste0(tn, "_fltr_vals")
  f <- sf_dta_read(sfc = sfc, tv = tn)
  f <- dt_dta_open(dta_link = f)
  if (isTRUE(nrow(f) > 0)) {
    f <- f[!(stage %in% ppsij$dtp_n[1] & step %in% ppsij$dt_n[1] &
          date_time > min(dt$date_time) & date_time < max(dt$date_time)
      )
    ]
  }
  zouts  <- data.table::rbindlist(lapply(zouts, function(x) x$zouts))
  f <- rbind(f, zouts)
  f <- f[order(phen, step, stage, date_time)]
  sf_dta_rm(sfc = sfc, rm = tn)
  dta_room <- file.path(dirname((sfc[1])), tn)
  class(f) <- c(class(f), "fltr_vals")
  sl <- sf_dta_wr(dta_room = dta_room, dta = f, overwrite = TRUE, tn = tn,
    rit = "event_based", ri = "discnt", chunk_v = chunk_v
  )
  sf_log_update(sfc = sfc, log_update = sl)
  ppsij <- ppsij[, ":="(start_dttm = dta_in_o[[1]][[hsf_dta]]$indx$mn,
      end_dttm = dta_in_o[[1]][[hsf_dta]]$indx$mx
    )
  ]
  return(list(ppsij = ppsij))
}